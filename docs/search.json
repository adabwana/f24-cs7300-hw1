[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homework 1",
    "section": "",
    "text": "Jaryt Salvo\nDate: 9/13/24\nFall 2024 | CS7300 Unsupervised Learning\n\nThis project contains solutions to Homework 1 from the Unsupervised Learning course (CS7300) using Clojure. The code provided here demonstrates the application of various unsupervised learning techniques, including dimensionality reduction, clustering, and feature extraction.\nThe code is organized into different sections corresponding to each homework problem, with detailed explanations of the algorithms and mathematical concepts involved. We utilize Clojure and its associated libraries, such as scicloj.clay for rendering, tablecloth for data manipulation, and fastmath for mathematical operations.\n\nUtils\nThe utils.clj file contains various utility functions and helpers used throughout the homework solutions. It includes:\n\nFormatting functions for markdown and LaTeX rendering\nData preprocessing and normalization functions\nVisualization functions for creating plots\nHelper functions for implementing unsupervised learning algorithms\n\nThese utilities are designed to streamline the implementation process and provide reusable components for data analysis and visualization in the context of unsupervised learning.\nThe code in the src/assignments folder was rendered with Clay and deployed with Github Pages.\nNote: Some interactive visualizations may not display correctly in the static HTML output. For the best experience, it’s recommended to run the code in a Clojure REPL environment.\nYou can add more content or import other namespaces as needed for your homework",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Jaryt Salvo</span>"
    ]
  },
  {
    "objectID": "assignments.hw1.utils.html",
    "href": "assignments.hw1.utils.html",
    "title": "Utils",
    "section": "",
    "text": "(ns assignments.hw1.utils\n  (:require\n   [scicloj.kindly.v4.api :as kindly]\n   [scicloj.kindly.v4.kind :as kind]\n   [tablecloth.api :as tc]\n   [tech.v3.datatype.functional :as dtf]\n   [uncomplicate.neanderthal\n    [core :refer [col entry nrm2 mv mrows ncols\n                  scal axpy copy mm dia trans]]\n    [native :refer [dge]]\n    [linalg :refer [ev! svd]]]))\n\nFormatting code\n\n(def md (comp kindly/hide-code kind/md))\n\n\n(def question (fn [content] ((comp kindly/hide-code kind/md) (str \"## \" content \"\\n---\"))))\n\n\n(def sub-question (fn [content] ((comp kindly/hide-code kind/md) (str \"#### *\" content \"*\"))))\n\n\n(def sub-sub (fn [content] ((comp kindly/hide-code kind/md) (str \"***\" content \"***\"))))\n\n\n(def formula (comp kindly/hide-code kind/tex))\n\n\n(def answer \n  (fn [content] \n    (kind/md \n     (str \"&gt; &lt;span style=\\\"color: black; font-size: 1.5em;\\\"&gt;**\" content \"**&lt;/span&gt;\"))))\n\nnormalize/standardize\n\n(defn tc-col-&gt;vec\n  [data col]\n  (vec (col (tc/select-columns data col))))\n\n\n(defn normalize-column\n  \"Normalize a column using min-max normalization\"\n  [col]\n  (let [min-val (apply min col)\n        max-val (apply max col)]\n    (map #(/ (- % min-val) (- max-val min-val)) col)))\n\nTODO: in neanderthal, use dge/mean and dge/std-dev\n\n(defn standardize-column\n  \"Standardize a column using z-score normalization\"\n  [col]\n  (let [mean (dtf/mean col)\n        std (dtf/standard-deviation col)]\n    (map #(/ (- % mean) std) col)))\n\n\n(defn apply-to-all-columns\n  \"Apply a function to all columns of a dataset\"\n  [data func]\n  (reduce (fn [acc col-name]\n            (tc/add-column acc col-name (func (col-name data))))\n          (tc/dataset)\n          (tc/column-names data)))\n\n\n(let [data (tc/dataset {:age [20 30 35 40 50]\n                        :income [30000 40000 59000 55000 90000]\n                        :credit-score [650 700 750 800 850]})\n\n      normalized-data (apply-to-all-columns data normalize-column)]\n  (tc/dataset normalized-data))\n\n\n_unnamed [5 3]:\n\n\n\n:age\n:income\n:credit-score\n\n\n\n\n0\n0\n0\n\n\n0.3333\n0.1667\n0.2500\n\n\n0.5000\n0.4833\n0.5000\n\n\n0.6667\n0.4167\n0.7500\n\n\n1\n1\n1\n\n\n\n\ncenter\n\n(defn matrix-&gt;dataset\n  \"Transforms a Neanderthal matrix into a Tablecloth dataset.\n   Columns are named x1 to xp, where p is the number of columns in the matrix.\"\n  [matrix]\n  (let [rows (mrows matrix)\n        cols (ncols matrix)\n        column-names (mapv #(keyword (str \"x\" (inc %))) (range cols))\n        data (for [i (range rows)]\n               (for [j (range cols)]\n                 (entry matrix i j)))]\n    (tc/dataset (map (fn [row] (zipmap column-names row)) data))))\n\n\n(defn dataset-&gt;matrix\n  \"Converts a Tablecloth dataset to a Neanderthal matrix.\"\n  [dataset]\n  (let [X (tc/rows dataset :as-seqs)]                       ;opt ':as-seq' is default\n    (dge (count X) (count (first X))                        ;{:layout :column} is default\n         (flatten X) {:layout :row})))\n\n\n(defn center-data\n  \"Centers the data by subtracting the mean of each column.\n   Parameters:\n   - data: Neanderthal matrix to be centered\"\n  [data]\n  (let [mean-vec (dge 1 (ncols data) (map #(/ (reduce + %) (mrows data)) (trans data)))\n        centered-data (axpy -1 (mm (dge (mrows data) 1 (repeat (mrows data) 1)) mean-vec) data)]\n    centered-data))\n\n\n(defn compute-covariance-matrix\n    \"Computes the covariance matrix of the centered data.\n    Parameters:\n    - centered-data: Centered Neanderthal matrix\"\n    [centered-data]\n    (mm (trans centered-data) centered-data))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utils</span>"
    ]
  },
  {
    "objectID": "assignments.hw1.q1.html",
    "href": "assignments.hw1.q1.html",
    "title": "Question 1",
    "section": "",
    "text": "(ns assignments.hw1.q1\n  (:require\n   [assignments.hw1.utils :refer :all]\n   [tablecloth.api :as tc]))\n\n\n\nQ1: A Case Analysis (40 Points)\nA data scientist is working on a machine learning project with a dataset that initially contains 10 features. However, the model performance was not satisfying based on initial experiments. To enhance the model’s performance, he decided to add 50 more features, thinking that more data points would improve the model’s accuracy. However, the opposite occurred: the model’s accuracy decreased, the training process became slower, and the results became less reliable and harder to interpret. By looking into the details of the data, the scientist noticed three things:\n\nThere were a lot of numerical features that vary significantly regarding values.\nThe model was overfitting with too many features.\nThe training process took much longer to converge (finish).\n\nCan you help the scientist identify the causes of the failure and propose some solutions?\nThe data scientist’s challenges stem from several key issues related to the nature of the features, model complexity, and data processing.\n1. Problem: Features with significant numerical variation****\nCause: Features that vary significantly in magnitude can negatively impact many machine learning models, especially those that rely on distance metrics (e.g., linear models, SVMs). Some features may dominate others due to their scale, leading to biased results.\nSolution: Apply feature scaling techniques like standardization (z-score normalization) or min-max normalization to ensure all features are on a similar scale. This will allow the model to weigh all features more equally.\n\n(let [data (tc/dataset {:age [20 30 35 40 50]\n                        :income [30000 40000 59000 55000 90000]\n                        :credit-score [650 700 750 800 850]\n                        :z [-2 -1 0 1 2]})\n\n      normalized-data (apply-to-all-columns data normalize-column)]\n  (tc/dataset normalized-data))\n\n\n_unnamed [5 4]:\n\n\n\n:age\n:income\n:credit-score\n:z\n\n\n\n\n0\n0\n0\n0\n\n\n0.3333\n0.1667\n0.2500\n0.2500\n\n\n0.5000\n0.4833\n0.5000\n0.5000\n\n\n0.6667\n0.4167\n0.7500\n0.7500\n\n\n1\n1\n1\n1\n\n\n\n\nI. Normalization (Min-Max Scaling):\nPurpose: Scales features to a fixed range, typically [0, 1].\nMathematics: Where X is the original value, X_min is the minimum value in the feature, and X_max is the maximum value. See utils/normalize-column for specifics.\nBenefits:\n\nBounds values to a specific range, useful when you need values in a certain range (e.g., for neural network inputs).\nPreserves zero values and sparsity in sparse data.\nHelpful when the distribution of data is not Gaussian or the standard deviation is unknown.\n\n\n(let [data (tc/dataset {:age [20 30 35 40 50]\n                        :income [30000 40000 59000 55000 90000]\n                        :credit-score [650 700 750 800 850]\n                        :z [-2 -1 0 1 2]}) ; results?\n\n      standardized-data (apply-to-all-columns data standardize-column)]\n  (tc/dataset standardized-data))\n\n\n_unnamed [5 4]:\n\n\n\n:age\n:income\n:credit-score\n:z\n\n\n\n\n-1.34164079\n-1.08473944\n-1.26491106\n-1.26491106\n\n\n-0.44721360\n-0.64734450\n-0.63245553\n-0.63245553\n\n\n0.00000000\n0.18370587\n0.00000000\n0.00000000\n\n\n0.44721360\n0.00874790\n0.63245553\n0.63245553\n\n\n1.34164079\n1.53963017\n1.26491106\n1.26491106\n\n\n\n\nII. Standardization (Z-score Normalization):\nPurpose: Transforms data to have a mean of 0 and a standard deviation of 1.\nMathematics: Where X is the original value, μ is the mean of the feature, and σ is the standard deviation.\nBenefits:\n\nCenters the feature around zero, which can be important for many machine learning algorithms.\nUseful when features have significantly different scales or units.\nParticularly beneficial for algorithms that assume normally distributed data (e.g., linear regression, logistic regression).\n\n2. Problem: Model overfitting due to too many features****\nCause: Adding more features increases the dimensionality of the dataset, which can lead to the curse of dimensionality. The model may become too complex and start to ‘memorize’ the training data rather than generalizing to new data, leading to overfitting.\nSolution: Reduce the number of features by applying dimensionality reduction techniques like Principal Component Analysis (PCA) or Feature Selection methods (e.g., Lasso regression, recursive feature elimination). This reduces complexity while keeping the most relevant information.\n\n(let [data (-&gt; (tc/dataset {:x [1 2 3]\n                            :y [10 8 2]\n                            :z [6 3 5]})\n               dataset-&gt;matrix)\n      centered-data (center-data data)]\n  centered-data)\n\n\n#RealGEMatrix[double, mxn:3x3, layout:row]\n   ▤       ↓       ↓       ↓       ┓    \n   →      -4.67    5.67    2.67         \n   →      -3.67    3.67   -0.33         \n   →      -2.67   -2.33    1.67         \n   ┗                               ┛    \n\n\n\n(comment\n  (defn compute-covariance-matrix\n    \"Computes the covariance matrix of the centered data.\n    Parameters:\n    - centered-data: Centered Neanderthal matrix\"\n    [centered-data]\n    (mm (trans centered-data) centered-data)))\n\n\n(let [data (-&gt; (tc/dataset {:x [1 2 3]\n                            :y [10 8 2]\n                            :z [6 3 5]})\n               dataset-&gt;matrix)\n      centered-data (center-data data)\n      cov-matrix (compute-covariance-matrix centered-data)]\n  cov-matrix)\n\n\n#RealGEMatrix[double, mxn:3x3, layout:column]\n   ▥       ↓       ↓       ↓       ┓    \n   →      42.33  -33.67  -15.67         \n   →     -33.67   51.00   10.00         \n   →     -15.67   10.00   10.00         \n   ┗                               ┛    \n\n\n3. Problem: Slower training process****\nCause: With more features, the computational complexity increases, leading to slower convergence of the model during training. This is especially true for iterative algorithms like gradient descent.\nSolution: Simplify the model by using fewer features (as mentioned) and apply regularization techniques like L1 (Lasso) or L2 (Ridge) to penalize overly complex models and prevent overfitting. Additionally, try early stopping methods to halt the training when improvement stagnates, preventing wasted computation time.\nFurther Considerations for Model Improvement:\n\nCross-validation: Implement k-fold cross-validation to enhance model performance assessment and mitigate overfitting risks.\nData augmentation: When feasible, expand the dataset through appropriate augmentation techniques. This approach can help the model better utilize the increased feature space and potentially reduce overfitting.\nModel complexity optimization: Begin with simpler, more interpretable models such as decision trees or linear regression before progressing to more complex architectures. This stepwise approach allows for:\n\nEstablishing a baseline performance\nIdentifying key features\nGradually increasing model complexity as needed\n\nComplex models often require more extensive hyperparameter tuning and computational resources, which may not always yield proportional performance gains.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Question 1</span>"
    ]
  },
  {
    "objectID": "assignments.hw1.q2.html",
    "href": "assignments.hw1.q2.html",
    "title": "Question 2",
    "section": "",
    "text": "(ns assignments.hw1.q2\n  (:require\n   [assignments.hw1.utils :refer :all]\n   [fastmath.core :as m]))\n\n\n\nQ2: Probability Review (40 Points)\nSuppose a certain disease affects 2% of the population. A test for the disease is 90% accurate for those who have the disease (true positive rate) and 95% accurate for those who do not have the disease (true negative rate). What is the probability that a person who tested positive has the disease?\nExplicitly write down the steps of the calculation. Result without steps shown will receive zero point.\nThis problem is a classic application of Bayes’ theorem, which allows us to calculate conditional probabilities. Bayes’ theorem is expressed as:\n\\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\]\nWhere: - P(A|B) is the probability of event A given that B has occurred - P(B|A) is the probability of event B given that A has occurred - P(A) is the probability of event A - P(B) is the probability of event B\nIn our context: - A is the event of having the disease - B is the event of testing positive\nBayes’ theorem allows us to calculate the probability of having the disease given a positive test result, which is not immediately apparent from the given information. This demonstrates the theorem’s power in updating probabilities based on new evidence.\nLet’s break down the solution step by step:\n\n(let [population 100000        disease-rate 0.02\n      true-positive-rate 0.90        true-negative-rate 0.95\n        ; Step 1: Calculate the number of people with and without the disease\n      with-disease (* population disease-rate)\n      without-disease (- population with-disease)\n        ; Step 2: Calculate true positives and false positives\n      true-positives (* with-disease true-positive-rate)\n      false-positives (* without-disease (- 1 true-negative-rate))\n        ; Step 3: Calculate total positive tests\n      total-positives (+ true-positives false-positives)\n        ; Step 4: Calculate the probability\n      probability (/ true-positives total-positives)]\n\n  (answer\n   (str \"Final probability that despite a positive test result, there is only about a \"\n        (* 100 (m/approx probability 4))\n        \"% chance that the person actually has the disease.\")))\n\n\nFinal probability that despite a positive test result, there is only about a 26.87% chance that the person actually has the disease.\n\nThis counterintuitive result, known as the base rate fallacy, occurs because the low prevalence of the disease in the population significantly influences the outcome, even with a highly accurate test.\n\n\nExplanation:\n\nWe start with a hypothetical population of 100,000 to make our calculations more tangible.\nGiven the disease rate of 2%, we calculate the number of people with and without the disease:\n\n\\[\\text{With disease} = 100,000 \\times 0.02 = 2,000\\]\n\\[\\text{Without disease} = 100,000 - 2,000 = 98,000\\]\n\nWe then determine the number of true positives (people with the disease who test positive) and false positives (people without the disease who test positive):\n\n\\[\\text{True positives} = 2,000 \\times 0.90 = 1,800\\]\n\\[\\text{False positives} = 98,000 \\times (1 - 0.95) = 4,900\\]\n\nThe total number of positive tests is the sum of true positives and false positives:\n\n\\[\\text{Total positives} = 1,800 + 4,900 = 6,700\\]\n\nFinally, we calculate the probability that a person who tested positive has the disease by dividing the number of true positives by the total number of positive tests:\n\n\\[P(\\text{Disease}|\\text{Positive}) = \\frac{\\text{True positives}}{\\text{Total positives}} = \\frac{1,800}{6,700} \\approx 0.2687\\]\nThis approach, reminiscent of Kahneman’s ‘Linda problem’, demonstrates how our intuitions about probability can be misleading. Despite the high accuracy rates of the test, the low prevalence of the disease in the population significantly affects the final probability.\nIt’s worth noting that this calculation assumes independence between test results and disease prevalence, which may not always hold in real-world scenarios. Factors such as test administration, population demographics, and environmental conditions could potentially influence these probabilities.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Question 2</span>"
    ]
  },
  {
    "objectID": "assignments.hw1.q3.html",
    "href": "assignments.hw1.q3.html",
    "title": "Question 3",
    "section": "",
    "text": "(ns assignments.hw1.q3\n  (:require\n   [assignments.hw1.utils :refer :all]\n   [tablecloth.api :as tc]\n   [fastmath.core :as m]\n   [fastmath.stats :as s]))\n\n\n\nQ3: Early Course Feedback (20 points)\nI hope the course is going smoothly for you. Based on the contents in the first two weeks, what do you feel? Are you struggling with any topics? Are the contents interesting and helpful to you? Are you expecting any changes in the class? Your feedback is important and helpful in improving the content quality and course delivery. Please do not hesitate to share.\nCourse Feedback:\nPositive Aspects: The course has been engaging so far, with several noteworthy strengths. Dr. Nui’s presentation style is nice, with well-organized information in PowerPoint slides and smooth talker during lectures. The course content is intriguing, with topics such as Recurrent Neural Networks (RNNs) standing out as particularly fascinating.\nAreas for Improvement: While the theoretical aspects are covered, I’d like to see more practical application of concepts. The inclusion of more code-based examples would be beneficial. For instance, implementing neural networks we’ve discussed (e.g. MLP, RNN, CNN) using PyTorch would help bridge the gap between theory and practice.\nSuggestions: To address these challenges, it would be helpful to incorporate more coding exercises and demonstrations. Step-by-step guides for implementing key algorithms or models discussed in class would be invaluable. This approach would both solidify our understanding of our topics and improve our ability to apply theoretical knowledge in practical scenarios.\nChallenges: Some concepts, particularly RNNs, have proven challenging to fully grasp. Moreover, translating course concepts into code has been difficult. A case in point is my attempt to implement PCA using eigenvectors in this assignment. This endeavor presented significant hurdles, especially when working with eigenvalues. Despite my efforts, I struggled to correctly calculate and apply the eigenvalues, which are crucial for determining the principal components.\nPersonal Goals: Moving forward, I aim to improve my ability to translate theoretical concepts into working code. Specifically, I plan to continue refining my PCA implementation, and make it work. In addition, I plan to implement K-means clustering–as another unsupervised method–that I feel is possible to implement in terms of a Clojure library for machine learning.\nOverall, the class is valuable and engaging. The integration of more hands-on coding examples would enhance the learning experience.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Question 3</span>"
    ]
  }
]